{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SPwr7LpS9-20Z78p4xabi8j7NOQiJtPe",
      "authorship_tag": "ABX9TyPUr5uhqBakSoA7+YXYNyE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgajendra/LanguageModels/blob/main/Attention_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRfOuenkFLbB",
        "outputId": "005d1020-b4df-47ed-9234-8c00307ee7dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/archive')\n",
        "os.listdir()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvUgZsXcFXor",
        "outputId": "08b6a15d-246a-478f-c136-ad80bebc8398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['movie_lines.txt',\n",
              " 'movie_titles_metadata.txt',\n",
              " 'movie_characters_metadata.txt',\n",
              " '.DS_Store',\n",
              " 'raw_script_urls.txt',\n",
              " 'README.txt',\n",
              " 'chameleons.pdf',\n",
              " 'movie_conversations.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('movie_lines.txt','r',encoding='utf-8',errors='ignore') as file:\n",
        "  lines = file.readlines()\n",
        "with open('movie_conversations.txt','r',encoding='utf-8',errors='ignore') as file:\n",
        "  convs = file.readlines()"
      ],
      "metadata": {
        "id": "PExhQys-KF0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[:10], convs[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMRGu7foK7Ey",
        "outputId": "3f2d013e-7a81-490b-f179-5c94c67bf7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n',\n",
              "  'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n',\n",
              "  'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n',\n",
              "  'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n',\n",
              "  \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\",\n",
              "  'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n',\n",
              "  \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\",\n",
              "  'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n',\n",
              "  'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n',\n",
              "  'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'],\n",
              " [\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\\n\",\n",
              "  \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\\n\"])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines_split={}\n",
        "convs_split=[]\n",
        "\n",
        "for line in lines:\n",
        "  line_split = line.split(' +++$+++ ')\n",
        "  lines_split[line_split[0].strip()] = line_split[-1].strip()\n",
        "\n",
        "for conv in convs:\n",
        "  conv_split = conv.split(' +++$+++ ')[-1][1:-2].replace(\"'\",\"\").replace(\", \",\" \").split()\n",
        "  convs_split.append(conv_split)"
      ],
      "metadata": {
        "id": "vNKHQcnmLT5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "def remove_punct(strings):\n",
        "  punctuation = string.punctuation\n",
        "  no_punct=\"\"\n",
        "  for char in strings:\n",
        "    if char not in punctuation:\n",
        "      no_punct=no_punct+char\n",
        "  return no_punct.lower()\n"
      ],
      "metadata": {
        "id": "1hY4BZSAZu3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "max_len = 25\n",
        "for conv in convs_split:\n",
        "  for c in range(len(conv)-1):\n",
        "    qa_pairs=[]\n",
        "\n",
        "    first = remove_punct(lines_split[conv[c]])\n",
        "    second = remove_punct(lines_split[conv[c+1]])\n",
        "\n",
        "    qa_pairs.append(first.split()[:max_len])\n",
        "    qa_pairs.append(second.split()[:max_len])\n",
        "    pairs.append(qa_pairs)\n"
      ],
      "metadata": {
        "id": "AnbibiuiL7RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glp22m4XS1MS",
        "outputId": "9dabaf88-915e-47c1-fd0c-5daf0dd0c508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221616"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "word_freq = Counter()\n",
        "min_word_freq = 5\n",
        "word_freq_trim = {}\n",
        "for pair in pairs:\n",
        "   word_freq.update(pair[0])\n",
        "   word_freq.update(pair[1])\n",
        "for word,freq in word_freq.items():\n",
        "  if word_freq[word] > min_word_freq:\n",
        "    word_freq_trim[word] = freq + 1\n",
        "#word_freq_trim is nothing but the bag of words"
      ],
      "metadata": {
        "id": "s_mqufEqZOY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##word dictionary\n",
        "word_indx = {}\n",
        "for i,word in enumerate(word_freq_trim.keys()):\n",
        "  word_indx[word] = i + 1\n",
        "word_indx['<unk>'] = len(word_indx) + 1\n",
        "word_indx['<start>'] = len(word_indx) + 1\n",
        "word_indx['<end>'] = len(word_indx) + 1\n",
        "word_indx['<pad>'] = 0\n",
        "\n",
        "indx_word={}\n",
        "for k, v in word_indx.items():\n",
        "  indx_word[v] = k"
      ],
      "metadata": {
        "id": "L1YAunS3YQ8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_questions(words, word_indx):\n",
        "  encode_q = [word_indx[word] if word in word_indx.keys() else word_indx['<unk>'] for word in words ] + [word_indx['<pad>']]*(max_len - len(words))\n",
        "  return encode_q"
      ],
      "metadata": {
        "id": "54OrxlE4nN35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_answers(words, word_indx):\n",
        "  encode_a = [word_indx['<start>']]+[word_indx[word] if word in word_indx.keys() else word_indx['<unk>'] for word in words] + [word_indx['<end>']] + [word_indx['<pad>']]*(max_len - len(words))\n",
        "  return encode_a"
      ],
      "metadata": {
        "id": "MXjyNv1kvuGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "  q_encoded = encode_questions(pair[0],word_indx)\n",
        "  a_encoded = encode_answers(pair[1],word_indx)\n",
        "  pairs_encoded.append([q_encoded,a_encoded])"
      ],
      "metadata": {
        "id": "v0kLsp_zyPtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class dataset(Dataset):\n",
        "  def __init__(self, input_pairs):\n",
        "    self.input_pairs = input_pairs\n",
        "    self.input_pairs_len = len(input_pairs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    question = torch.LongTensor(self.input_pairs[index][0])\n",
        "    answer = torch.LongTensor(self.input_pairs[index][1])\n",
        "\n",
        "    return question, answer\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.input_pairs_len"
      ],
      "metadata": {
        "id": "onQAHMd312MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#covert list to tensor of (batch_size, max_len), [100, 25]\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader =  DataLoader(dataset(pairs_encoded), batch_size=100, shuffle=True)"
      ],
      "metadata": {
        "id": "SibhsYeX3e1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##A look-ahead mask is required to prevent the decoder from attending to succeeding words, \n",
        "##such that the prediction for a particular word can only depend on known outputs for the words \n",
        "##that come before it.\n",
        "##https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/\n",
        "\n",
        "def create_mask(question, answers_input, answers_target):\n",
        "  \n",
        "  def look_ahead_mask(size):\n",
        "    mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
        "    return mask\n",
        "  \n",
        "  question_mask = (question != 0).to(device)\n",
        "  question_mask = question_mask.unsqueeze(1).unsqueeze(1) #[100,1,1,25]\n",
        "\n",
        "  answer_input_mask = (answers_input != 0).to(device)\n",
        "  answer_input_mask = answer_input_mask.unsqueeze(1)\n",
        "  answer_input_mask = answer_input_mask & look_ahead_mask(answers_input.size(-1)).type_as(answer_input_mask.data)\n",
        "\n",
        "  answers_target_mask = (answers_target!=0)\n",
        "\n",
        "  return question_mask, answer_input_mask, answers_target_mask"
      ],
      "metadata": {
        "id": "saIwXn889RW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class embedding(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, d_model, max_len=50):\n",
        "    super(embedding,self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.embed = nn.Embedding(vocab_size, d_model)\n",
        "    self.pe = self.create_positional_encoding(max_len, d_model)\n",
        "\n",
        "  def create_positional_encoding(self, max_len, d_model):\n",
        "    pe = torch.zeros(max_len, d_model).to(device)\n",
        "    for pos in range(max_len): ##For each position of the word\n",
        "      for i in range(0,d_model,2):\n",
        "        pe[pos, i] = math.sin(pos/(10000** ((2*i)/d_model)))\n",
        "        pe[pos, i+1] = math.cos(pos/(10000** ((2*(i+1))/d_model)))\n",
        "    pe = pe.unsqueeze(0)\n",
        "    return pe\n",
        "\n",
        "  def forward(self, encoded_words):\n",
        "    embeddings = self.embed(encoded_words)*math.sqrt(self.d_model)\n",
        "    embeddings += self.pe[:, :embeddings.size(1)]\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "ODbgoBR9NMN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class multi_head_attention(nn.Module):\n",
        "  def __init__(self,heads, d_model):\n",
        "    super(multi_head_attention,self).__init__()\n",
        "    self.heads = heads\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.d_k = d_model//heads\n",
        "    self.query = nn.Linear(d_model, d_model)\n",
        "    self.key = nn.Linear(d_model, d_model)\n",
        "    self.value = nn.Linear(d_model, d_model)\n",
        "    self.concat = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, query, key, value,mask):\n",
        "    \"\"\"\n",
        "    q, k, v of shape (batch_size, max_words, d_model)\n",
        "    mask: (batch_size, 1, 1, max_words)\n",
        "    \"\"\"\n",
        "    query = self.query(query)   #(batch_size, max_words, d_model)\n",
        "    key = self.key(key)         #(batch_size, max_words, d_model)\n",
        "    value = self.value(value)   #(batch_size, max_words, d_model)\n",
        "\n",
        "    ##query should of shape \n",
        "    #--> (batch_size, max_words, d_model)-->(batch_size, max_words, h, dk) --> (batch_size, h, max_word, dk)\n",
        "    query = query.view(query.size(0), -1, self.heads, self.d_k)\n",
        "    #swap dim 2 and dim 1\n",
        "    query = query.permute(0,2,1,3)\n",
        "\n",
        "    key = key.view(query.size(0), -1, self.heads, self.d_k)\n",
        "    #swap dim 2 and dim 1\n",
        "    key = key.permute(0,2,1,3)\n",
        "\n",
        "    value = value.view(query.size(0), -1, self.heads, self.d_k)\n",
        "    #swap dim 2 and dim 1\n",
        "    value = value.permute(0,2,1,3)\n",
        "\n",
        "    ##now take dor product of query and key\n",
        "    #(batch_size, h, max_word, dk) * (batch_size, h, dk, max_word)  --> (batch_size, h, max_word, max_word)\n",
        "    score = torch.matmul(query, key.permute(0,1,3,2))/math.sqrt(self.d_k)\n",
        "    #masking\n",
        "    score = score.masked_fill(mask==0, -1e9)\n",
        "    #softmax\n",
        "    weights = nn.softmax(score, dim=-1)  #last dimension max_len, which is drived from key, \n",
        "    weights = self.dropout(weights)\n",
        "\n",
        "    #now dor prodouct with the value\n",
        "    #(batch_size, h, max_word, max_word) * (batch_size, h, max_word, dk)  --> (batch_size, h, max_word, dk)\n",
        "    context = torch.matmul(weights, value)\n",
        "    #later we will transpose the matrix\n",
        "    #(batch_size, h, max_word, dk)  --> (batch_size, max_word, h, dk) --> (batch_size, max_len, h*dk)(concat all h to get back 512 embedding)\n",
        "    context = context.permute(0,2,1,3).view(context.size(0),-1, self.heads * self.dk)\n",
        "    interacted = self.concat(context)\n",
        "\n",
        "    return interacted"
      ],
      "metadata": {
        "id": "wWwxcAMLEYoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class feedforward(nn.Module):\n",
        "  def __init__(self,d_model, middle_dim=2048):\n",
        "    super(feedforward,self).__init__()\n",
        "    self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "\n",
        "  def forward(self, interacted):\n",
        "    feed = F.relu(self.fc1(interacted))\n",
        "    feed = self.dropout(feed)\n",
        "    feed = self.fc2(feed)\n",
        "    return feed"
      ],
      "metadata": {
        "id": "ePblIOTxx5n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder_layer(nn.Module):\n",
        "  def __init__(self, d_model, heads):\n",
        "    super(encoder_layer,self).__init__()\n",
        "    self.self_multiheadattention = multi_head_attention(heads, d_model)\n",
        "    self.feedforward = feedforward(d_model)\n",
        "    self.layerNorm = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, embeddings, mask):\n",
        "    interacted = self.self_multiheadattention(embeddings,embeddings,embeddings,mask)\n",
        "    interacted = self.layerNorm(interacted + embeddings)\n",
        "    feed_forward_out = self.dropout(self.feedforward(interacted))\n",
        "    encoded = self.layerNorm(feed_forward_out+interacted)\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "k_KBBxSpaXTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class decoder_layer(nn.Module):\n",
        "  def __init__(self, d_model, heads):\n",
        "    super(decoder_layer,self).__init__()\n",
        "    self.self_multiheadattention = multi_head_attention(heads, d_model)\n",
        "    self.src_multiheadattention = multi_head_attention(heads, d_model)\n",
        "    self.feedforward = feedforward(d_model)\n",
        "    self.layerNorm = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "    query = self.self_multiheadattention(embeddings,embeddings,embeddings,target_mask)\n",
        "    query = self.dropout(query)\n",
        "    query = self.layerNorm(query+embedding)\n",
        "    interacted = self.src_multiheadattention(query,encoded,encoded,src_mask)\n",
        "    interacted = self.dropout(interacted)\n",
        "    interacted = self.layerNorm(interacted+query)\n",
        "    feed_forward_out = self.dropout(self.feedforward(interacted))\n",
        "    decoded = self.layerNorm(feed_forward_out + interacted)\n",
        "    return decoded\n",
        "    "
      ],
      "metadata": {
        "id": "FtPGpDTBhe9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer, 6encoder layer and 6decoder\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, d_model, heads, num_layers, word_map):\n",
        "    super(Transformer,self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = len(word_map)\n",
        "    self.embed = embedding(self.vocab_size, d_model)\n",
        "    self.encoder = nn.ModuleList([encoder_layer(d_model, heads) for _ in range(num_layers)])\n",
        "    self.decoder = nn.ModuleList([decoder_layer(d_model, heads) for _ in range(num_layers)])\n",
        "    self.logit = nn.Linear(d_model, self.vocab_size)\n",
        "\n",
        "\n",
        "  def encode(self, src_words, src_mask):\n",
        "    src_embeddings =  self.embed(src_words)\n",
        "    for layers in self.encoder:\n",
        "      src_embeddings = layers(src_embeddings,src_mask)\n",
        "    return src_embeddings\n",
        "\n",
        "  def decode(self,target_words, target_mask,src_embeddings, src_mask):\n",
        "    tgt_embeddings =  self.embed(target_words)\n",
        "    for layers in self.decoder:\n",
        "      tgt_embeddings = layers(tgt_embeddings, src_embeddings,src_mask, target_mask)\n",
        "    return tgt_embeddings\n",
        "\n",
        "  def forward(self, src_word, src_mask, target_words, target_mask):\n",
        "    encoded = self.encode(src_word,src_mask)\n",
        "    decoded = self.decode(target_words, target_mask,encoded, src_mask)\n",
        "    out = F.log_softmax(self.logit(decoded))\n",
        "    return out"
      ],
      "metadata": {
        "id": "NmqIU0Uh3gvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adamwarmup:\n",
        "  def __init__(self, model_size, warmup_steps, optimizer):\n",
        "    self.model_size = model_size\n",
        "    self.warmup_steps = warmup_steps\n",
        "    self.optimizer = optimizer\n",
        "    self.current_step = 0\n",
        "    self.lr = 0\n",
        "\n",
        "  def get_lr(self):\n",
        "     return self.model_size ** (-0.5) * min(self.current_step **(-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "\n",
        "  def step(self):\n",
        "    self.current_step += 1\n",
        "    lr = self.get_lr()\n",
        "    for param_group in self.optimizer.param_groups:\n",
        "      param_group['lr'] = lr\n",
        "    self.lr = lr\n",
        "    #update weights\n",
        "    self.optimizer.step()   #update the weights\n"
      ],
      "metadata": {
        "id": "58EddruFQ_tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class losswithLS(nn.Module):\n",
        "  def __init__(self, size, smoth):\n",
        "    super(losswithLS, self).__init__()\n",
        "    self.criteria = nn.KLDivLoss(size_average = False, reduce = False)\n",
        "    self.confidence =1-smoth\n",
        "    self.smooth = smoth\n",
        "    self.size = size\n",
        "\n",
        "  def forward(self, prediction, target, mask):\n",
        "    \"\"\"\n",
        "    prediction: (batch_size, max_words, vocab_size)\n",
        "    target and mask: (batch_size, max_words)\n",
        "    \"\"\"\n",
        "    \n",
        "    predictions = prediction.view(-1, prediction.size(-1))\n",
        "    #label smoothing\n",
        "    target = target.view(-1)\n",
        "    mask = mask.float()\n",
        "    mask = mask.view(-1)\n",
        "    labels = predictions.data.clone()  ##copy the labels\n",
        "    labels.fill_(self.smooth/(self.size-1))\n",
        "    labels.scatter(1, target.data.unsqueeze(1), self.confidence)\n",
        "\n",
        "    #build the loss\n",
        "    loss = self.criteria(predictions, labels)\n",
        "    loss = (loss.sum(1) * mask).sum()/mask.sum()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Fu0HIoKabXMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "heads =8\n",
        "num_layers =1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs =1\n",
        "word_map = word_indx\n",
        "\n",
        "transformer = Transformer(d_model=d_model, heads=heads, num_layers=num_layers, word_map=word_map)\n",
        "transformer.to(device)\n",
        "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr = 0,betas=(0.9,0.98), eps=1e-9)\n",
        "tranformer_optimer = Adamwarmup(model_size=d_model, warmup_steps=4000, optimizer=adam_optimizer)\n",
        "criterion = losswithLS(size=len(word_map), smoth=0.3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_eZuSt_5Q74",
        "outputId": "3a3a0af9-727f-4481-9bfe-f7c8dbdd2efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, transformer, criterion, epoch):\n",
        "  \n",
        "  transformer.train()\n",
        "  sum_loss = 0\n",
        "  count = 0\n",
        "  for i, (question, reply) in enumerate(train_loader):\n",
        "\n",
        "    samples = question.shape[0]\n",
        "    #move to device\n",
        "    question = question.to(device)\n",
        "    reply = reply.to(device)\n",
        "\n",
        "    reply_input = reply[:, :-1]\n",
        "    reply_target = reply[:, 1:]\n",
        "\n",
        "    question_mask, reply_input_mask, reply_target_mask = create_mask(question, reply_input, reply_target)\n",
        "\n",
        "    #run through transformer to get predictions\n",
        "    out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "    #loss\n",
        "    loss = criterion(out, reply_target, reply_target_mask)\n",
        "\n",
        "    ##backprop\n",
        "    tranformer_optimer.optimizer.zero_grads()\n",
        "\n",
        "    loss.backward()\n",
        "    tranformer_optimer.step()\n",
        "\n",
        "    sum_loss += loss.item()*samples\n",
        "    count += 1\n",
        "\n",
        "    if i % 100 ==0:\n",
        "      print(\"Epochs [{}][{}/{}]\\tLoss: {:.3f}\".format(epochs, i, len(train_loader), sum_loss/count))"
      ],
      "metadata": {
        "id": "y_h0n69S5wXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Evaluation(transformer, question, question_mask, max_len, word_indx):\n",
        "  rev_indx_word = {v:k for k, v in word_indx.items()}\n",
        "  transformer.eval()\n",
        "\n",
        "  start_token = word_indx[\"<start>\"]\n",
        "  encoded = transformer.encoded(question, question_mask)\n",
        "  words = torch.LongTensor([[start_token]]).to(device)\n",
        "\n",
        "  for step in range(max_len -1):\n",
        "    size = words.shape[0]\n",
        "    target_mask = torch.triu(torch.ones(size,size)).transpose(0,1).type(dtype=torch.uint8)\n",
        "    target_mask = target_mask.to(device).unsqueeze(0)\n",
        "    decoded = transformer.decoded(words, target_mask,encoded, question_mask)\n",
        "    #decoded --> (1, 1,vocab_size)\n",
        "    predictions = transformer.logit([decoded[:,-1]])\n",
        "    _, next_word = torch.max(predictions, dim=1)\n",
        "    next_word = next_word.item()\n",
        "    if next_word == word_indx[\"<end>\"]:\n",
        "      break\n",
        "    words = torch.cat(words, torch.LongTensor([[next_word]]).to(device),dim=1)\n",
        "\n",
        "  words = words.sequeeze(0) #once dimenision tensor\n",
        "  words = words.tolist()\n",
        "\n",
        "  sen_indx = [ w for w in words if w not in {word_indx[\"<start>\"]}]\n",
        "  sentence = \" \".join([rev_indx_word[sen_indx[k]] for k in range(len(sen_indx))])\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "Ve3D1N68m-A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  train(train_loader, transformer, criterion, epoch)\n",
        "  state = {'epoch':epoch, 'transformer':transformer,'transformer_optimizer':tranformer_optimer}\n",
        "  torch.save(state,'checkpoint_'+str(epoch)+'.tar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "f75AwHSS-TFU",
        "outputId": "d5b9d0d7-ac09-4c51-a6ae-56e54280c825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-116-e0c50ecc4e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transformer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'transformer_optimizer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtranformer_optimer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'checkpoint_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-4c85921f1545>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, transformer, criterion, epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#run through transformer to get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply_input_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m#loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply_target_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-9c4c72c99b6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_word, src_mask, target_words, target_mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-9c4c72c99b6d>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, src_words, src_mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msrc_embeddings\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0msrc_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msrc_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-19fe73791e37>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0minteracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_multiheadattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0minteracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteracted\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfeed_forward_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteracted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-106-7784ec9c2f45>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m#softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#last dimension max_len, which is drived from key,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'softmax'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('checkpoint_.tar')\n",
        "transformer = checkpoint['transformer']"
      ],
      "metadata": {
        "id": "nzVhAnPWi1Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(1):\n",
        "  question=input(\"Question:\")\n",
        "  if question=='quit':\n",
        "    break\n",
        "  max_len=input(\"Enter max word to be granted\")\n",
        "  enc_ques = [word_map.get(word, word_indx['unk']) for word in question.split()]\n",
        "  question = torch.LongTensor(enc_ques).to(device).unsqueeze(0)\n",
        "  question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
        "  sentence = evaluate(transformer, question, question_mask, max_len, word_indx)\n",
        "  print(sentence)\n"
      ],
      "metadata": {
        "id": "VxyrjfWa_Hyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question='How are you'\n",
        "question.split()"
      ],
      "metadata": {
        "id": "qCYbI8Wk_2uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a[:, :-1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haTAbIrpi6Fg",
        "outputId": "b72df670-42ed-4baa-87b9-ee4ea76293a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[:, 1:].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plKKwmx_jOA-",
        "outputId": "b6484d85-4d2e-4c23-9d83-96090d7f12b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTBqtVLEjPpl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}